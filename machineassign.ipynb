{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization constant\n",
    "\n",
    "The concept of a normalizing constant arises in probability theory and a variety of other areas of mathematics. The normalizing constant is used to reduce any probability function to a probability density function with total probability of one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance:\n",
    "Covariance provides a measure of the strength of the correlation between two or more sets of random variates. The covariance for two random variates X and Y, each with sample size N, is defined by the expectation value\n",
    "The covariance formula is similar to the formula for correlation and deals with the calculation of data points from the average value in a dataset. For example, the covariance between two random variables X and Y can be calculated using the following formula \n",
    "                            $$ğ¶ğ‘œğ‘£(ğ‘¥,ğ‘¦)=âˆ‘ğ‘–=0ğ‘›(ğ‘¥ğ‘–âˆ’ğ‘¥Â¯)(ğ‘¦ğ‘—âˆ’ğ‘¦Â¯)ğ‘›$$\n",
    "\n",
    "For a sample covariance, the formula is slightly adjusted:\n",
    "\n",
    "$$ Cov(x,y) =  \\sum_{i=0}^n \\frac {({x_{i}- \\bar{x}})({y_{j}- \\bar{y}})}{n - 1}$$<br>\n",
    "                            \n",
    "where:  ğ‘¥ğ‘–  = the values of the x-variable \n",
    "\n",
    "ğ‘¦ğ‘–  = the values of the y-variable \n",
    "\n",
    "ğ‘¥Â¯  = the mean (average) of the x-variable \n",
    "\n",
    "ğ‘¦Â¯  = the mean (average) of the y-variable \n",
    "\n",
    "n = the number of the data points\n",
    "\n",
    "n - 1 = degree of freedom\n",
    "\n",
    "Degrees of freedom is the number of independent data points that went into calculating the estimate. As we see in the example above, it is not necessarily equal to the number of items in the sample (n).\n",
    "Covariance is used to analyze the linear relationship between the two variable.The positive and negative value in covariance matrix denotes the increasing and decreasing relationship between the two variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain what degrees of freedom are, let us just take an example. In a set of 3 numbers with the mean as 10 and two out of three variables as 5 and 15, there is only one possibility of the value that the third number can take up i.e. 10. With any set of 3 numbers with the same mean, for example: 12,8 and 10 or say 9,10 and 11, there is only one value for any 2 given values in the set. You can basically change the two values here and the third value fixes itself. The degree of freedom here is 2. Essentially, degrees of freedom is the number of independent data points that went into calculating the estimate. As we see in the example above, it is not necessarily equal to the number of items in the sample (n).\n",
    "\n",
    "The diagonal of a covariance matrix provides the variance of each individual variable, covariance itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of Covariance Matrix\n",
    "Covariance matrices are always positive semidefinite. To see why, let X be any random vector with covariance matrix Î£, and let b be any constant row vector. Define the random variable\n",
    "$$ğ¶ğ‘¥=ğ¸[(ğ‘‹âˆ’ğ‘š)(ğ‘‹âˆ’ğ‘š)ğ‘‡]$$\n",
    "X is a random variable\n",
    "E(X) = Î¼ is the expected value (the mean) of the random variable X and\n",
    "E(Y) = Î½ is the expected value (the mean) of the random variable Y\n",
    "n = the number of items in the data set\n",
    "The variance of any random variable Y must be nonnegative, so expression is nonnegative. symmetric matrix Î£ is positive semidefinite if bÎ£bâ€² â‰¥ 0 for all row vectors b. A covariance matrix is necessarily symmetric, so we conclude that all covariance matrices Î£ are positive semidefinite.\n",
    "\n",
    "We shall call a random vector nonsingular or singular according to whether its covariance matrix is positive definite or singular positive semidefinite.\n",
    "\n",
    "The (i, j)thelement of this covariance matrixCxis given by\n",
    "$$Cij=E[(Xiâˆ’mi)(Xjâˆ’mj)] =Ïƒij$$\n",
    "                  \n",
    "Since the covariance matrixCxis symmetric, i.e., self-adjoint with the usualinner product its eigenvalues are all real and positive and the eigenvectors thatbelong to distinct eigenvalues are orthogonal,\n",
    "$$Cx=VÎ›VT=nâˆ‘i=1Î»i~vi~vTi$$\n",
    "As a consequence, the determinant of the covariance matrix is positive,\n",
    "$$Det(CX) =nâˆi=1Î»iâ‰¥0$$\n",
    "The eigenvectors of the covariance matrix transform the random vector intostatistically uncorrelated random variables, i.e., into a random vector with adiagonal covariance matrix. \n",
    "The Rayleigh coefficient of the covariance matrixis bounded above and below by the maximum and minimum eigenvalue $$Î»minâ‰¤aTCxaaTa,aâˆˆRâ‰¤Î»max.1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias of an estimator:\n",
    "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased. Otherwise the estimator is said to be biased. In statistics, \"bias\" is an objective statement about a function, and while not a desired property, it is not pejorative, unlike the ordinary English use of the term \"bias\".\n",
    "\n",
    "Bias can also be measured with respect to the median, rather than the mean (expected value), in which case one distinguishes median-unbiased from the usual mean-unbiasedness property. Bias is related to consistency in that consistent estimators are convergent and asymptotically unbiased (hence converge to the correct value), though individual estimators in a consistent sequence may be biased (so long as the bias converges to zero)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
